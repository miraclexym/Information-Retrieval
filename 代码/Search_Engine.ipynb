{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载倒排索引...\n",
      "倒排索引加载完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import jieba\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# 增加字段大小限制，设置为更大的值（比如 10MB）\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "# 读取倒排索引\n",
    "def load_index(filename=\"Text_Index.pkl\"):\n",
    "    print(\"加载倒排索引...\")\n",
    "    with open(filename, 'rb') as f:\n",
    "        inverted_index, doc_tfs, idf = pickle.load(f)\n",
    "    print(\"倒排索引加载完成。\")\n",
    "    return inverted_index, doc_tfs, idf\n",
    "\n",
    "# 打开并读取 Html_Content.csv 文件\n",
    "csv_filename = \"Html_Content.csv\"\n",
    "\n",
    "# 读取文件中的内容\n",
    "with open(csv_filename, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    # 跳过表头（如果有的话）\n",
    "    next(csv_reader, None)\n",
    "    # 将所有内容读入列表\n",
    "    html_data = list(csv_reader)\n",
    "\n",
    "# 加载倒排索引\n",
    "inverted_index, doc_tfs, idf = load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# 中文分词\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "# 计算PageRank的函数（采用简化的PageRank算法）\n",
    "def compute_pagerank(html_folder, top_docs, d=0.85, max_iter=100, tol=1e-6):\n",
    "    # 构建网页之间的链接关系\n",
    "    link_graph = defaultdict(list)\n",
    "    doc_urls = []\n",
    "    for doc_id, score in top_docs:\n",
    "        # 获取网页的URL和锚文本\n",
    "        webpage_title = html_data[doc_id][0]    # 第一列是 标题\n",
    "        webpage_url = html_data[doc_id][1]      # 第二列是 URL\n",
    "        doc_urls.append(webpage_url)\n",
    "        webpage_anchors = html_data[doc_id][2]  # 第三列是 锚文本\n",
    "        \n",
    "        # 构建本地文件路径\n",
    "        local_file_path = os.path.join(html_folder, f\"{webpage_title}.html\")\n",
    "        \n",
    "        # 从本地HTML文件中解析链接\n",
    "        if os.path.exists(local_file_path):\n",
    "            with open(local_file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                links = extract_links_from_html(content)  # 提取HTML中的链接\n",
    "                for link in links:\n",
    "                    # 如果该链接在我们关注的网页列表中，则记录下来\n",
    "                    if link in doc_urls:\n",
    "                        link_graph[webpage_url].append(link)\n",
    "    \n",
    "    # 初始化PageRank\n",
    "    num_docs = len(doc_urls)\n",
    "    pagerank = {url: 1 / num_docs for url in doc_urls}  # 每个页面的初始PageRank值为 1/N\n",
    "    for _ in range(max_iter):\n",
    "        new_pagerank = {}\n",
    "        for url in doc_urls:\n",
    "            inbound_links = [key for key, links in link_graph.items() if url in links]\n",
    "            rank_sum = sum(pagerank[link] / len(link_graph[link]) for link in inbound_links)\n",
    "            new_pagerank[url] = (1 - d) / num_docs + d * rank_sum\n",
    "        \n",
    "        # 检查是否收敛\n",
    "        if all(abs(new_pagerank[url] - pagerank[url]) < tol for url in doc_urls):\n",
    "            break\n",
    "        \n",
    "        pagerank = new_pagerank\n",
    "    \n",
    "    return pagerank\n",
    "\n",
    "# 从HTML中提取所有的链接（简化版本，仅提取href链接）\n",
    "def extract_links_from_html(content):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    return [link['href'] for link in links]\n",
    "\n",
    "# 计算查询的TF-IDF向量\n",
    "def compute_query_tfidf(query, idf):\n",
    "    query_tokens = tokenize(query)\n",
    "    query_tf = defaultdict(int)\n",
    "    \n",
    "    # 计算TF\n",
    "    for word in query_tokens:\n",
    "        query_tf[word] += 1\n",
    "    total_words = len(query_tokens)\n",
    "    for word in query_tf:\n",
    "        query_tf[word] /= total_words\n",
    "    \n",
    "    # 计算TF-IDF\n",
    "    query_tfidf = {}\n",
    "    for word, tf in query_tf.items():\n",
    "        query_tfidf[word] = tf * idf.get(word, 0)  # 默认IDF为0\n",
    "    \n",
    "    return query_tfidf\n",
    "\n",
    "# 计算文档与查询的相关度（余弦相似度）\n",
    "def compute_cosine_similarity(query_tfidf, doc_tfidf):\n",
    "    dot_product = sum(query_tfidf.get(word, 0) * doc_tfidf.get(word, 0) for word in query_tfidf)\n",
    "    query_norm = math.sqrt(sum(val**2 for val in query_tfidf.values()))\n",
    "    doc_norm = math.sqrt(sum(val**2 for val in doc_tfidf.values()))\n",
    "    if query_norm * doc_norm == 0:\n",
    "        return 0\n",
    "    return dot_product / (query_norm * doc_norm)\n",
    "\n",
    "# 执行搜索\n",
    "def search(query, inverted_index, doc_tfs, idf, top_n=10):\n",
    "    \n",
    "    # 计算查询的TF-IDF向量\n",
    "    query_tfidf = compute_query_tfidf(query, idf)\n",
    "    \n",
    "    # 计算每个文档与查询的相关度\n",
    "    scores = []\n",
    "    for doc_id, doc_tfidf in enumerate(doc_tfs):\n",
    "        score = compute_cosine_similarity(query_tfidf, doc_tfidf)\n",
    "        scores.append((doc_id, score))\n",
    "    \n",
    "    # 按相关度排序并返回前top_n个文档\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_docs = scores[:100]\n",
    "    \n",
    "    # 计算PageRank\n",
    "    html_folder = './HtmlFile/'\n",
    "    pagerank = compute_pagerank(html_folder, top_docs)\n",
    "    \n",
    "    # 综合文档评分与PageRank\n",
    "    combined_scores = []\n",
    "    for doc_id, score in top_docs:\n",
    "        webpage_url = html_data[doc_id][1]  # 获取文档URL\n",
    "        pagerank_score = pagerank.get(webpage_url, 0)  # 获取该网页的PageRank\n",
    "        combined_score = score + pagerank_score  # 综合评分\n",
    "        combined_scores.append((doc_id, combined_score))\n",
    "    \n",
    "    # 按综合评分重新排序并返回前top_n个文档\n",
    "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_combined_docs = combined_scores[:top_n]\n",
    "    \n",
    "    return top_combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义循环次数计数器\n",
    "search_count = 0\n",
    "\n",
    "# 初始化查询日志\n",
    "query_log = []\n",
    "\n",
    "# html 文件存放在当前目录下的 HtmlFile 文件夹\n",
    "html_folder = './HtmlFile/'\n",
    "\n",
    "# 确保 HtmlFile 文件夹存在\n",
    "if not os.path.exists(html_folder):\n",
    "    print(\"HtmlFile 文件夹不存在，请检查文件路径。\")\n",
    "    exit()\n",
    "\n",
    "# 定义一个用于过滤标点符号的函数\n",
    "def is_valid_word(word):\n",
    "    invalid_symbols = ['，', '%', '‰', '、']\n",
    "    invalid_words = ['与', '本', '的', '由', '多', '1', '新']\n",
    "    \n",
    "    if any(symbol in word for symbol in invalid_symbols) or word in invalid_words:\n",
    "        return False\n",
    "    \n",
    "    return all(char not in string.punctuation for char in word)\n",
    "\n",
    "# 分词并统计双词\n",
    "def extract_bigrams(text):\n",
    "    words = list(jieba.cut(text))  # 切割文本\n",
    "    bigrams = []\n",
    "    for i in range(len(words) - 1):\n",
    "        bigram = f\"{words[i]}{words[i+1]}\"\n",
    "        if is_valid_word(words[i]) and is_valid_word(words[i+1]) and words[i] != words[i+1]:\n",
    "            bigrams.append(bigram)\n",
    "    return bigrams\n",
    "\n",
    "# 读取用户信息\n",
    "def load_user_info(file_path):\n",
    "    users = {}\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for row in reader:\n",
    "                account, password, nickname, preferences = row\n",
    "                users[account] = {'password': password, 'nickname': nickname, 'preferences': preferences}\n",
    "    return users\n",
    "\n",
    "# 保存用户信息\n",
    "def save_user_info(file_path, users):\n",
    "    with open(file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for account, info in users.items():\n",
    "            writer.writerow([account, info['password'], info['nickname'], info['preferences']])\n",
    "\n",
    "# 用户注册\n",
    "def register(users):\n",
    "    print(\"用户注册\")\n",
    "    account = input(\"请输入账号: \")\n",
    "    if account in users:\n",
    "        print(\"账户已存在，请选择其他账户名。\")\n",
    "        return None\n",
    "    password = input(\"请输入密码: \")\n",
    "    nickname = input(\"请输入昵称: \")\n",
    "    preferences = input(\"请输入您的偏好设置 (例如: '学术, 科技'): \")\n",
    "    users[account] = {'password': password, 'nickname': nickname, 'preferences': preferences}\n",
    "    print(f\"注册成功！欢迎，{nickname}\")\n",
    "    return account\n",
    "\n",
    "# 用户登录\n",
    "def login(users):\n",
    "    print(\"用户登录\")\n",
    "    account = input(\"请输入账号: \")\n",
    "    if account not in users:\n",
    "        print(\"账号不存在，请先注册。\")\n",
    "        return None\n",
    "    password = input(\"请输入密码: \")\n",
    "    if users[account]['password'] == password:\n",
    "        print(f\"登录成功！欢迎，{users[account]['nickname']}\")\n",
    "        return account, users[account]['preferences']\n",
    "    else:\n",
    "        print(\"密码错误，请重新尝试。\")\n",
    "        return None\n",
    "\n",
    "# 主程序\n",
    "def main():\n",
    "    # 加载用户信息\n",
    "    user_info_file = 'User_Information.csv'\n",
    "    users = load_user_info(user_info_file)\n",
    "    \n",
    "    # 注册或登录\n",
    "    current_user = None\n",
    "    preferences = None\n",
    "    while current_user is None:\n",
    "        action = input(\"请选择操作: 1. 注册 2. 登录 (输入 'exit' 退出): \")\n",
    "        if action == '1':\n",
    "            current_user = register(users)\n",
    "            preferences = users[current_user]['preferences'] if current_user else None\n",
    "        elif action == '2':\n",
    "            result = login(users)\n",
    "            if result:\n",
    "                current_user, preferences = result\n",
    "        elif action.lower() == 'exit':\n",
    "            print(\"退出程序\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"无效选项，请选择 1 或 2。\")\n",
    "    \n",
    "    # 保存用户信息\n",
    "    save_user_info(user_info_file, users)\n",
    "    \n",
    "    # 初始化查询日志\n",
    "    query_log.clear()\n",
    "    \n",
    "    # 循环进行查询\n",
    "    global search_count\n",
    "    while True:\n",
    "        # 根据循环次数决定搜索内容\n",
    "        if search_count == 0:\n",
    "            query = \"南开大学ESI学科发展报告pdf\"  # 文档查询测试\n",
    "        elif search_count == 1:\n",
    "            query = \"南开大学新校区\"  # 短语查询测试\n",
    "        elif search_count == 2:\n",
    "            query = \"新冠肺炎防控*漫画\"  # 通配查询测试\n",
    "        elif search_count == 3:\n",
    "            query = \"exit\"\n",
    "        else:\n",
    "            query = input(\"请输入查询词（输入 'exit' 退出）：\")\n",
    "        # query = input(\"请输入查询词（输入 'exit' 退出）：\")\n",
    "\n",
    "        # 如果用户输入 \"exit\"，退出循环\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"\\n退出程序。\")\n",
    "            break\n",
    "        \n",
    "        # 输出查询内容\n",
    "        print(f\"\\n正在搜索：{query}\")\n",
    "        \n",
    "        # 记录查询日志\n",
    "        query_log.append(query)\n",
    "        \n",
    "        # 如果用户已登录，则将用户偏好附加到查询词\n",
    "        if preferences:\n",
    "            query = f\"{query} {preferences}\"\n",
    "\n",
    "        # 执行搜索（假设search是一个已定义的搜索函数）\n",
    "        top_docs = search(query, inverted_index, doc_tfs, idf, top_n=10)\n",
    "        \n",
    "        # 输出搜索结果\n",
    "        print(\"搜索结果：\")\n",
    "        \n",
    "        all_bigrams = []  # 用于收集所有搜索结果中的双词\n",
    "        \n",
    "        for doc_id, score in top_docs:\n",
    "            webpage_title = html_data[doc_id][0]    # 假设第一列是 标题\n",
    "            webpage_url = html_data[doc_id][1]      # 假设第二列是 URL\n",
    "            webpage_anchors = html_data[doc_id][2]  # 假设第三列是 锚文本\n",
    "            webpage_body = html_data[doc_id][3]     # 假设第四列是 正文\n",
    "\n",
    "            all_bigrams.extend(extract_bigrams(webpage_anchors))\n",
    "            all_bigrams.extend(extract_bigrams(webpage_body))\n",
    "\n",
    "            # 构建本地文件路径（假设网页标题对应文件名）\n",
    "            local_file_path = os.path.join(html_folder, f\"{webpage_title}.html\")\n",
    "            \n",
    "            if os.path.exists(local_file_path):\n",
    "                local_file_path = local_file_path.replace(' ', '%20')\n",
    "                local_file_url = f\"file:///{os.path.abspath(local_file_path)}\"\n",
    "                print(f\"网页标题: {webpage_title}, 网页链接: {webpage_url}\\n本地文件链接: {local_file_url}\")\n",
    "            else:\n",
    "                print(f\"网页标题: {webpage_title}, 网页链接: {webpage_url}\\n本地文件不存在\")\n",
    "        \n",
    "        # 输出查询日志\n",
    "        print(\"\\n查询日志：\")\n",
    "        for i, log in enumerate(query_log, 1):\n",
    "            # if log.endswith(preferences):\n",
    "            #     log = log[:-len(preferences)]\n",
    "            print(f\"{i}. {log}\")\n",
    "        \n",
    "        # 输出当前用户的昵称和偏好\n",
    "        if current_user:\n",
    "            print(f\"\\n当前用户: {users[current_user]['nickname']}\")\n",
    "            print(f\"用户偏好: {preferences}\")\n",
    "\n",
    "        # 统计双词\n",
    "        bigram_counts = Counter(all_bigrams)\n",
    "        common_bigrams = bigram_counts.most_common(10)\n",
    "        print(\"\\n推荐搜索：\")\n",
    "        \n",
    "        # 输出推荐搜索短语\n",
    "        for i in range(0, len(common_bigrams), 10):\n",
    "            line = \"、\".join([bigram for bigram, _ in common_bigrams[i:i+10]])\n",
    "            print(line)\n",
    "        \n",
    "        search_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户登录\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ASUS\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "登录成功！欢迎，徐亚民\n",
      "\n",
      "正在搜索：南开大学ESI学科发展报告pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.538 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "搜索结果：\n",
      "网页标题: 南开大学ESI全球前1%学科增至15个 “微生物学”首次上榜-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2021/03/28/030045131.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学ESI全球前1%学科增至15个%20“微生物学”首次上榜-南开要闻-南开大学.html\n",
      "网页标题: 南开大学“社会科学总论”学科首次进入全球前1%-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2020/05/18/030039232.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学“社会科学总论”学科首次进入全球前1%-南开要闻-南开大学.html\n",
      "网页标题: 南开大学工程科学跻身ESI全球前1‰-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2023/11/25/030059003.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学工程科学跻身ESI全球前1‰-南开要闻-南开大学.html\n",
      "网页标题: 南开大学环境科学与生态学跻身ESI全球前1‰-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2024/07/17/030062633.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学环境科学与生态学跻身ESI全球前1‰-南开要闻-南开大学.html\n",
      "网页标题: 中央广电总台国际在线：南开大学环境科学与生态学跻身ESI全球前1‰-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2024/07/18/030062653.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\中央广电总台国际在线：南开大学环境科学与生态学跻身ESI全球前1‰-媒体南开-南开大学.html\n",
      "网页标题: 中宏网：南开大学环境科学与生态学跻身ESI全球前1‰-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2024/07/18/030062657.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\中宏网：南开大学环境科学与生态学跻身ESI全球前1‰-媒体南开-南开大学.html\n",
      "网页标题: 南开大学ESI学科取得双突破-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2020/03/14/030038236.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学ESI学科取得双突破-南开要闻-南开大学.html\n",
      "网页标题: 我校新增植物与动物科学学科进入ESI全球前1%-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2019/03/27/000439857.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\我校新增植物与动物科学学科进入ESI全球前1%-南开要闻-南开大学.html\n",
      "网页标题: 中国发展网：南开大学ESI全球前1%学科增至11个 分子生物学与遗传学“入列”-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2018/05/18/000383271.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\中国发展网：南开大学ESI全球前1%学科增至11个%20分子生物学与遗传学“入列”-媒体南开-南开大学.html\n",
      "网页标题: 【1367期】学校召开学科建设工作会议-南开大学报-南开大学, 网页链接: http://news.nankai.edu.cn/nkdxb/system/2018/08/08/000404334.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\【1367期】学校召开学科建设工作会议-南开大学报-南开大学.html\n",
      "\n",
      "查询日志：\n",
      "1. 南开大学ESI学科发展报告pdf\n",
      "\n",
      "当前用户: 徐亚民\n",
      "用户偏好: 信息检索\n",
      "\n",
      "推荐搜索：\n",
      "被引、南开要闻、进入ESI、环境科学、媒体南开、全球前、南开大学报、ESI全球、南开校史、光影南开\n",
      "\n",
      "正在搜索：南开大学新校区\n",
      "搜索结果：\n",
      "网页标题: 城市快报：南开天大新校区啥模样-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2010/03/10/000029044.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\城市快报：南开天大新校区啥模样-媒体南开-南开大学.html\n",
      "网页标题: 【1208期】薛进文调研新校区建设-南开大学报-南开大学, 网页链接: http://news.nankai.edu.cn/nkdxb/system/2013/10/25/000148444.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\【1208期】薛进文调研新校区建设-南开大学报-南开大学.html\n",
      "网页标题: 【新校区】薛进文调研津南新校区施工现场-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2013/09/26/000142067.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\【新校区】薛进文调研津南新校区施工现场-南开要闻-南开大学.html\n",
      "网页标题: 天津人民广播电台：传承保护并重，南开大学6月中下旬启动搬迁，天南大北洋园新校区9月全面投入使用-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2015/05/20/000235062.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\天津人民广播电台：传承保护并重，南开大学6月中下旬启动搬迁，天南大北洋园新校区9月全面投入使用-媒体南开-南开大学.html\n",
      "网页标题: 城市快报：南开新校区正式启用-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2015/09/06/000246904.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\城市快报：南开新校区正式启用-媒体南开-南开大学.html\n",
      "网页标题: 新华社：南开大学新校区９月新学期投入使用-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2015/08/29/000246315.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\新华社：南开大学新校区９月新学期投入使用-南开要闻-南开大学.html\n",
      "网页标题: 汉院学生寒假社会实践聚焦新校区治理-综合新闻-南开大学, 网页链接: http://news.nankai.edu.cn/zhxw/system/2015/03/09/000224435.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\汉院学生寒假社会实践聚焦新校区治理-综合新闻-南开大学.html\n",
      "网页标题: 每日新报：天大南大这个暑假 “搬新家”(图)-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2015/01/08/000216915.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\每日新报：天大南大这个暑假%20“搬新家”(图)-媒体南开-南开大学.html\n",
      "网页标题: 津南校区通勤公交陆续开通-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2015/09/11/000247774.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\津南校区通勤公交陆续开通-南开要闻-南开大学.html\n",
      "网页标题: 每日新报：海河教育园天南大新校区组团式布局 宿舍有空调-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2013/11/22/000156087.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\每日新报：海河教育园天南大新校区组团式布局%20宿舍有空调-媒体南开-南开大学.html\n",
      "\n",
      "查询日志：\n",
      "1. 南开大学ESI学科发展报告pdf\n",
      "2. 南开大学新校区\n",
      "\n",
      "当前用户: 徐亚民\n",
      "用户偏好: 信息检索\n",
      "\n",
      "推荐搜索：\n",
      "校区建设、津南校区、媒体南开、南开要闻、南开大学报、南开校史、光影南开、南开故事、校史网、南开师生\n",
      "\n",
      "正在搜索：新冠肺炎防控*漫画\n",
      "搜索结果：\n",
      "网页标题: 南开大学教师主译 《新冠肺炎防控指南漫画双语系列》推出-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2020/07/04/030040017.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学教师主译%20《新冠肺炎防控指南漫画双语系列》推出-南开要闻-南开大学.html\n",
      "网页标题: 持续科研攻关 为打赢疫情防控阻击战贡献南开力量-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2020/03/15/030038251.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\持续科研攻关%20为打赢疫情防控阻击战贡献南开力量-南开要闻-南开大学.html\n",
      "网页标题: 中国科学报：免疫在新冠肺炎发病和治疗中起何作用-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2020/04/27/030038948.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\中国科学报：免疫在新冠肺炎发病和治疗中起何作用-媒体南开-南开大学.html\n",
      "网页标题: 杨树山：因为我爱漫画-多彩校园-南开大学, 网页链接: http://news.nankai.edu.cn/dcxy/system/2014/11/19/000209515.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\杨树山：因为我爱漫画-多彩校园-南开大学.html\n",
      "网页标题: 我校参加全国教育系统疫情防控工作视频会议-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2022/03/15/030050600.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\我校参加全国教育系统疫情防控工作视频会议-南开要闻-南开大学.html\n",
      "网页标题: 南开大学与推想科技利用CT影像AI筛查助力新冠肺炎疫情防控-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2020/03/12/030038105.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\南开大学与推想科技利用CT影像AI筛查助力新冠肺炎疫情防控-南开要闻-南开大学.html\n",
      "网页标题: 光明日报：美国政府应对疫情的“三不”及其带来的人权灾难-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2020/10/22/030041336.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\光明日报：美国政府应对疫情的“三不”及其带来的人权灾难-媒体南开-南开大学.html\n",
      "网页标题: 我校参加教育部秋季学期开学和秋冬季疫情防控工作视频会-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2020/08/31/030040605.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\我校参加教育部秋季学期开学和秋冬季疫情防控工作视频会-南开要闻-南开大学.html\n",
      "网页标题: 学校召开会议部署强化新冠肺炎疫情防控工作-南开要闻-南开大学, 网页链接: http://news.nankai.edu.cn/ywsd/system/2021/08/04/030047483.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\学校召开会议部署强化新冠肺炎疫情防控工作-南开要闻-南开大学.html\n",
      "网页标题: 大公报：内地高教展明起举行-媒体南开-南开大学, 网页链接: http://news.nankai.edu.cn/mtnk/system/2013/11/22/000156078.shtml\n",
      "本地文件链接: file:///d:\\Programs\\InformationRetrieval\\HtmlFile\\大公报：内地高教展明起举行-媒体南开-南开大学.html\n",
      "\n",
      "查询日志：\n",
      "1. 南开大学ESI学科发展报告pdf\n",
      "2. 南开大学新校区\n",
      "3. 新冠肺炎防控*漫画\n",
      "\n",
      "当前用户: 徐亚民\n",
      "用户偏好: 信息检索\n",
      "\n",
      "推荐搜索：\n",
      "新冠肺炎、疫情防控、肺炎疫情、新冠病毒、南开要闻、媒体南开、肺炎防控、防控工作、防控指南、指南漫画\n",
      "\n",
      "退出程序。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
